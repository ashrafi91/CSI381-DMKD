{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1vGYD2ypcscD7sDCeZzR2wIbJ1QhzAeVM","timestamp":1637004933574},{"file_id":"1fLv1gcATGVkqVJyJ1LxxLk5jWQXQHYt-","timestamp":1636476360510},{"file_id":"18ncMGtY2tQL80wUapsVSCNHN41Qtrw1Y","timestamp":1635535081110}],"collapsed_sections":["H5JSY8HVvK9U","E2L1j6tY5LfG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HWRzJ7L7Poq5"},"source":["# **CSI 382 - Data Mining and Knowledge Discovery**"]},{"cell_type":"markdown","metadata":{"id":"szfybLmlPsj1"},"source":["# **Lab 8 - Hierarchical and k-means clustering**\n","\n","Clustering refers to the grouping of records, observations, or cases into classes of similar objects. A cluster is a collection of records that are similar to one another and dissimilar to records in other clusters.\n","\n","Clustering differs from classification in that there is no target variable for clustering. The clustering task does not try to classify, estimate, or predict the value of a target variable.\n","\n","Instead, clustering algorithms seek to segment the entire data set into relatively homogeneous subgroups or clusters, where the similarity of the records within the cluster is maximized, and the similarity to records outside this cluster is minimized."]},{"cell_type":"markdown","metadata":{"id":"bCtu167FQEU4"},"source":["# **Dataset for Lab 8**\n","\n","**Data Set Information:**\n","\n","This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis . We will demonstrate this by using unsupervised ML technique (KMeans Clustering Algorithm) and Hierarchical Clustering (Aggolomerative Methods) in the simplest form.\n","\n","**Content**\n","\n","You are owing a supermarket mall and through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score.\n","Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\n","\n","**Problem Statement**\n","\n","You own the mall and want to understand the customers like who can be easily converge [Target Customers] so that the sense can be given to marketing team and plan the strategy accordingly.\n","\n","\n","**Acknowledgement**\n","\n","From Udemy's Machine Learning A-Z course.\n","\n","\n","The dataset can be found here in this [URL](https://drive.google.com/file/d/1MJVGc9vg0mRa9pSrmJy93D9YkFODs6GR/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"t4yZD0OacRNd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Fh8U31FLDD0"},"source":["## **Loading the dataset**"]},{"cell_type":"code","metadata":{"id":"cpBGKLVWQBwV"},"source":["import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ifg8k1DIXMK"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/CSI 382 - Datasets/Mall_Customers.csv')\n","\n","#Check number of rows and columns in the dataset\n","print(\"The dataset has %d rows and %d columns.\" % df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OeX4ktpfYvIn"},"source":["# **Application of Clustering**"]},{"cell_type":"markdown","metadata":{"id":"TggfiVKRYkBV"},"source":["Examples of clustering tasks in business and research include:\n","* Target marketing of a niche product for a small-capitalization business that\n","does not have a large marketing budget\n","* For accounting auditing purposes, to segment financial behavior into benign\n","and suspicious categories\n","* As a dimension-reduction tool when a data set has hundreds of attributes\n","* For gene expression clustering, where very large quantities of genes may\n","exhibit similar behavior\n","\n","Clustering is often performed as a preliminary step in a data mining process,\n","with the resulting clusters being used as further inputs into a different technique downstream, such as neural networks. Due to the enormous size of many\n","present-day databases, it is often helpful to apply clustering analysis first, to reduce the search space for the downstream algorithms."]},{"cell_type":"markdown","metadata":{"id":"NF9dDiBmcMmN"},"source":["# **Selecting Data for clustering analysis**"]},{"cell_type":"markdown","metadata":{"id":"gla0WEa_cTXb"},"source":["Here we are selecting data attributes for clustering analysis. For the sake of simplicity we are selecting only the following attributes:\n","\n","* Annual Income\n","* Spending score"]},{"cell_type":"code","metadata":{"id":"K0F8G-plI27_"},"source":["X = df.iloc[:, [3, 4]].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uv-DYb3HZDIW"},"source":["# **Issues in clustering**"]},{"cell_type":"markdown","metadata":{"id":"quKinNyPZEr3"},"source":["Cluster analysis encounters many of the same issues that we dealt with in the\n","chapters on classification. For example, we shall need to determine:\n","* How to measure similarity\n","* How to recode categorical variables\n","* How to standardize or normalize numerical variables\n","* How many clusters we expect to uncover"]},{"cell_type":"code","metadata":{"id":"uQ9dzJG9JJ6Z"},"source":["from sklearn.cluster import KMeans\n","# (Within-Cluster Sum of Square)\n","wcss = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n","    kmeans.fit(X)\n","    wcss.append(kmeans.inertia_)\n","plt.plot(range(1, 11), wcss)\n","plt.title('Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WD_Vb8_rdHYh"},"source":["As we can see in the above figure, the above plot is visualized as a hand and we need to identify the location of the elbow on the X-axis. In the above plot, the elbow seems to be on point 5 of X-axis. So, the optimal number of clusters will be 5 for the K-Means algorithm."]},{"cell_type":"markdown","metadata":{"id":"YPzOSlrxWPyd"},"source":["# **Hierarchical Clustering Methods**"]},{"cell_type":"markdown","metadata":{"id":"2QY70kpGWa9y"},"source":["In hierarchical clustering, a treelike cluster structure (dendrogram) is created\n","through recursive partitioning (divisive methods) or combining (agglomerative)\n","of existing clusters."]},{"cell_type":"code","metadata":{"id":"f43JyINYJKVA"},"source":["import scipy.cluster.hierarchy as sch\n","\n","plt.figure(figsize=(10,10))\n","dendrogram = sch.dendrogram(sch.linkage(X, method = 'complete'))\n","plt.title('Dendrogram')\n","plt.xlabel('Customers')\n","plt.ylabel('Euclidean distances')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xo8guFgosM6k"},"source":["# **Aggolomerative Clustering**"]},{"cell_type":"markdown","metadata":{"id":"r5PGH3QLWjgi"},"source":["Agglomerative clustering methods [1] initialize each observation to be a tiny\n","cluster of its own. Then, in succeeding steps, the two closest clusters are aggre-\n","gated into a new combined cluster. In this way, the number of clusters in the data\n","set is reduced by one at each step. Eventually, all records are combined into a\n","single huge cluster.\n","\n","Divisive clustering methods begin with all the records in one big cluster, with\n","the most dissimilar records being split off recursively, into a separate cluster,\n","until each record represents its own cluster.\n","\n","Because most computer programs that apply hierarchical clustering use agglomerative methods, we focus on those."]},{"cell_type":"markdown","metadata":{"id":"ACm7vlZnWubo"},"source":["## **Single linkage**\n","\n","Single linkage, sometimes termed the nearest-neighbor approach, is based on\n","the minimum distance between any record in cluster A and any record in cluster\n","B. In other words, cluster similarity is based on the similarity of the most\n","similar members from each cluster.\n","Single linkage tends to form long, slender clusters, which may sometimes lead\n","to heterogeneous records being clustered together."]},{"cell_type":"code","metadata":{"id":"0-4zQnfkJKa-"},"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'single')\n","\n","y_hc = hc.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WaV5H91JKgz"},"source":["# Visualising the clusters\n","\n","plt.figure(figsize=(8,8))\n","plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careful Customers')\n","plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard Customers')\n","plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Luxury Customers')\n","plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careless Customers')\n","plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible Customers')\n","plt.title('Clusters of customers using Hierarchical Clustering')\n","plt.xlabel('Annual Income (K$)')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4M6NheKW0lU"},"source":["## **Complete linkage**\n","\n","Complete linkage, sometimes termed the farthest-neighbor approach, is based\n","on the maximum distance between any record in cluster A and any record in\n","cluster B.In other words, cluster similarity is based on the similarity of the\n","most dissimilar members from each cluster.\n","Complete-linkage tends to form more compact, sphere-like clusters, with all\n","records in a cluster within a given diameter of all other records."]},{"cell_type":"code","metadata":{"id":"Ebbs8LqLepgR"},"source":["from sklearn.cluster import AgglomerativeClustering\n","hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'complete')\n","y_hc = hc.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moJtgYC1epgU"},"source":["# Visualising the clusters\n","\n","plt.figure(figsize=(8,8))\n","plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Luxury Customers')\n","plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard Customers')\n","plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Careful Customers')\n","plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careless Customers')\n","plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible Customers')\n","plt.title('Clusters of customers using Hierarchical Clustering')\n","plt.xlabel('Annual Income (K$)')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-03BzCWW8lT"},"source":["## **Average linkage**\n","\n","Average linkage is designed to reduce the dependence of the cluster-linkage criterion on extreme values, such as the most similar or dissimilar records. In\n","average linkage, the criterion is the average distance of all the records in\n","cluster A from all the records in cluster B.\n","The resulting clusters tend to have approximately equal within-cluster variability."]},{"cell_type":"code","metadata":{"id":"4ck0E1kgeqNl"},"source":["from sklearn.cluster import AgglomerativeClustering\n","hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'average')\n","y_hc = hc.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0xbTpTIneqNn"},"source":["# Visualising the clusters\n","\n","plt.figure(figsize=(8,8))\n","plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Sensible Customers')\n","plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard and Careful Customers')\n","plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Luxury Customers')\n","plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careless Customers')\n","plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Somewhat Luxury Customers')\n","plt.title('Clusters of customers using Hierarchical Clustering')\n","plt.xlabel('Annual Income (K$)')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6o94Aaxde4gc"},"source":["## **Ward linkage**\n","\n","Ward´s linkage is a method for hierarchical cluster analysis . The idea has much in common with analysis of variance (ANOVA). The linkage function specifying the distance between two clusters is computed as the increase in the \"error sum of squares\" (ESS) after fusing two clusters into a single cluster. Ward´s Method seeks to choose the successive clustering steps so as to minimize the increase in ESS at each step.\n","\n"]},{"cell_type":"code","metadata":{"id":"gvAu4dmne4ge"},"source":["from sklearn.cluster import AgglomerativeClustering\n","hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\n","y_hc = hc.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2JD4Lwse4gf"},"source":["# Visualising the clusters\n","\n","plt.figure(figsize=(8,8))\n","plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careful Customers')\n","plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard Customers')\n","plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Luxury Customers')\n","plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Careless Customers')\n","plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible Customers')\n","plt.title('Clusters of customers using Hierarchical Clustering')\n","plt.xlabel('Annual Income (K$)')\n","plt.ylabel('Spending Score (1-100)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lOpFIMvXQv8"},"source":["# **k-means Clustering**"]},{"cell_type":"markdown","metadata":{"id":"pYp-jZ6uXPn8"},"source":["K-means [2] defines a prototype in terms of a centroid, which is usually the\n","mean of a group of points, and is typically applied to objects in a continuous\n","n-dimensional space. Interestingly, a centroid almost never corresponds to an\n","actual data point. In this section, we will focus on K-means, which is one of the\n","oldest and most widely used clustering algorithms."]},{"cell_type":"markdown","metadata":{"id":"UttEq1cGXXGm"},"source":["The k-means clustering algorithm [2] is a straightforward and effective algo-\n","rithm for finding clusters in data. The algorithm proceeds as follows.\n","* Step 1: Ask the user how many clusters $k$ the data set should be partitioned\n","into.\n","* Step 2: Randomly assign $k$ records to be the initial cluster center locations.\n","* Step 3: For each record, ﬁnd the nearest cluster center. Thus, in a sense, each cluster center “owns” a subset of the records, thereby representing a partition of the data set. We therefore have $k$ clusters, $C_1 , C_2 , \\dots , C_k$.\n","* Step 4: For each of the $k$ clusters, find the cluster centroid, and update the\n","location of each cluster center to the new value of the centroid.\n","* Step 5: Repeat steps 3 to 5 until convergence or termination."]},{"cell_type":"markdown","metadata":{"id":"hBIOoZXUX73N"},"source":["The “nearest” criterion in step 3 is usually Euclidean distance, although other\n","criteria may be applied as well. The cluster centroid in step 4 is found as follows. Suppose that we have $n$ data points $(a_1 , b_1 , c_1 ), (a_2 , b_2 , c_2 ), \\dots , (a_n , b_n , c_n )$, the centroid of these points is the center of gravity of these points and is located at point $(\\sum{a_i /n}, \\sum{b_i /n}, \\sum{c_i /n})$.\n","\n","For example, the points (1,1,1), (1,2,1), (1,3,1), and (2,1,1) would have centroid\n","$\\left( \\frac{1+1+1+2}{4}, \\frac{1+2+3+1}{4}, \\frac{1+1+1+1}{4} \\right) = (1.25, 1.75, 1.00)$\n","\\end{frame}"]},{"cell_type":"markdown","metadata":{"id":"0cTR-8EWYGL6"},"source":["The algorithm terminates when the centroids no longer change. In other words,\n","the algorithm terminates when for all clusters $C_1 , C_2 , \\dots , C_k $, all the records “owned” by each cluster center remain in that cluster. Alternatively, the algorithm may terminate when some convergence criterion is met, such as no signiﬁcant shrinkage in the sum\n","of squared errors:\n","\n","${SSE} = \\sum_{i=1}^{k}{\\sum_{p \\in C_i}{d(p, m_i)^2}}$\n","\n","where ${p \\in C_i}$ represents each data point in cluster $i$ and $m_i$ represents the centroid\n","of cluster $i$."]},{"cell_type":"code","metadata":{"id":"sBcW6TotJKHt"},"source":["kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n","y_kmeans = kmeans.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9WgvW5yJKPH"},"source":["plt.figure(figsize=(8,8))\n","plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Careful Customers')\n","plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Sensible Customers')\n","plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Standard Customers')\n","plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Careless Customers')\n","plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Luxury Customers')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'black')\n","plt.title('Clusters of Customers using K-Means Clustering')\n","plt.xlabel('Annual Income (K$)')\n","plt.ylabel('Spend Score (1-100)')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EeRoWerIdScr"},"source":["The name of clusters is given based on their income and spending. For example, when referring to a customer with low income and high spending, we have used cyan colour. This group indicates ‘Careless Customer’ since despite having a low income, they spend more. To sell a luxurious product, a person with high income and high spending habits should be targeted. This group of customers is represented in magenta colour in the above diagram."]},{"cell_type":"markdown","metadata":{"id":"8wlboB2iYMwt"},"source":["# **That's all for today!**"]},{"cell_type":"markdown","metadata":{"id":"3ktiGpE-0SKO"},"source":["# **Tasks**\n","\n","This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. More info about this dataset can be found here - [URL](https://archive-beta.ics.uci.edu/ml/datasets/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition).\n","\n","Download the dataset from here - [Download Link](https://drive.google.com/file/d/1QnIf-e7bCKBaKRfCPHmKwqcyiuoA_HD4/view?usp=sharing)\n","\n","\n","Now try to do the following:\n","\n","1. First find the number of optimal clusters using the elbow method\n","1. Apply k-means clusturing. Try different configurations to see what you get.\n","2. Apply aggolomerative clusturing method. Try different criteria of distance and do a comparative analysis on them."]}]}